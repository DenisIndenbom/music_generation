{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iA9OFlDAhmvS"
      },
      "source": [
        "# Импорт библиотек"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Te_rWzKmDN3j"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import sys\n",
        "from collections import OrderedDict\n",
        "from datetime import datetime\n",
        "from typing import Sequence, overload, Union, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mzlwl-U-o45x"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import itertools\n",
        "import os\n",
        "import sys\n",
        "from fractions import Fraction\n",
        "\n",
        "from music21 import chord, duration, instrument, note, stream\n",
        "from fractions import Fraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdZfffsVSQot"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twmFuYJ7dQzf"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = 'dataset/generation_music_dataset/processed_midi'\n",
        "VOCAB_DIR = 'vocabs'\n",
        "DATASET_DIR = 'dataset'\n",
        "CHECKPOINT_DIR = 'checkpoints/VAE'\n",
        "GENERATED_DIR = 'generated/VAE'\n",
        "\n",
        "PROCESS_MIDI_FILES = False\n",
        "\n",
        "SPLIT_DATASET = False\n",
        "BUILD_DATASET = False\n",
        "\n",
        "CHANNEL_NAMES = ('note', 'chord', 'duration', 'shift')\n",
        "\n",
        "MAX_SEQ_LEN = 128\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "BASE_LEARNING_RATE = 1e-5\n",
        "LEARNING_RATE = BASE_LEARNING_RATE * BATCH_SIZE / 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25l45vy8qfU0"
      },
      "outputs": [],
      "source": [
        "START_EPOCH = 107\n",
        "EPOCH_COUNT = 200"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s58aNFZ0OkFe"
      },
      "source": [
        "# Объявление базовых методов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98Bx6YSApTHH"
      },
      "outputs": [],
      "source": [
        "def split_dataset(path: str, val_prop=0.1):\n",
        "    df = pd.DataFrame([x for x in os.listdir(path) if x.endswith('abc')], columns=['filename'])\n",
        "    df['split'] = 'train'\n",
        "\n",
        "    df.loc[np.random.default_rng(42).choice(len(df), int(val_prop * len(df)), replace=False), 'split'] = 'val'\n",
        "    df.to_csv(path + '.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JI7gegaogQa8"
      },
      "outputs": [],
      "source": [
        "def load(path: str, max_seq_len, drop_last, filename: str):\n",
        "        path2abc = os.path.join(path, filename)\n",
        "        df = pd.read_csv(path2abc, names=CHANNEL_NAMES, dtype=str)\n",
        "\n",
        "        if max_seq_len < 0:\n",
        "            return [df.values]\n",
        "        else:\n",
        "            slices = []\n",
        "            for start in range(0, len(df), max_seq_len):\n",
        "                end = start + max_seq_len\n",
        "                if end > len(df):\n",
        "                    if drop_last:\n",
        "                        break\n",
        "\n",
        "                    end = len(df)\n",
        "\n",
        "                slices.append(df.iloc[start:end].values)\n",
        "\n",
        "            return slices\n",
        "\n",
        "def load_processed_data(path: str, *, split='train', max_seq_len: int = -1, drop_last: bool = True) -> list:\n",
        "    index = pd.read_csv(path + '.csv')\n",
        "    index = index[index.split == split]['filename'].values\n",
        "\n",
        "    data = list()\n",
        "    from multiprocessing import Pool, cpu_count\n",
        "    with Pool(processes=cpu_count()*4) as pool:\n",
        "        for slices in tqdm(pool.imap_unordered(functools.partial(load, path, max_seq_len, drop_last), index), total=len(index)):\n",
        "            for val in slices:\n",
        "                data.append(val)\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7fUc0mZgsqM"
      },
      "outputs": [],
      "source": [
        "def make_midi(x: torch.Tensor, vocab_map: OrderedDict):\n",
        "    midi_stream = stream.Stream()\n",
        "\n",
        "    x = x.detach().cpu().numpy()\n",
        "    next_offset = None\n",
        "    for line in x:\n",
        "        if np.any(line <= 1):\n",
        "            continue\n",
        "\n",
        "        p, c, d, s = (v[int(x)] for v, x in zip(vocab_map.values(), line))\n",
        "\n",
        "        if p == '<rest>':\n",
        "            element = note.Rest()\n",
        "        else:\n",
        "            if c == '0':\n",
        "                element = note.Note(p)\n",
        "            else:\n",
        "                p = note.pitch.Pitch(p).midi\n",
        "                note_list = [\n",
        "                    note.Note(p + int(x)) for x in c.split('-')\n",
        "                ]\n",
        "                element = chord.Chord(note_list)\n",
        "\n",
        "            element.storedInstrument = instrument.Piano()\n",
        "\n",
        "        element.duration = duration.Duration(Fraction(d))\n",
        "\n",
        "        midi_stream.append(element)\n",
        "        if next_offset is not None:\n",
        "            element.offset = next_offset\n",
        "            next_offset = None\n",
        "\n",
        "        if s != 'f':\n",
        "            next_offset = Fraction(element.offset) + Fraction(s)\n",
        "\n",
        "    return midi_stream"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHyxne9rO0Du"
      },
      "outputs": [],
      "source": [
        "def get_frequency_elements(elements: list, threshold: int = 50) -> list:\n",
        "    vocab = {}\n",
        "\n",
        "    for row in elements:\n",
        "        vocab[row] = vocab.get(row, 0) + 1\n",
        "\n",
        "    return list(x for _, x in sorted((-f, x) for x, f in vocab.items() if f >= threshold))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0lpjb_7CLmG"
      },
      "outputs": [],
      "source": [
        "class Vocab:\n",
        "    def __init__(self, elements: Optional[Sequence[str]], *, unk: str = '<unk>', num_special: Optional[int] = None):\n",
        "        self.int_to_element = tuple(elements)\n",
        "        self.element_to_int = dict(zip(self.int_to_element, range(len(self.int_to_element))))\n",
        "        self.unk_id = self.element_to_int[unk]\n",
        "        if num_special is None:\n",
        "            num_special = self.unk_id + 1\n",
        "\n",
        "        self.num_special = num_special\n",
        "\n",
        "    @overload\n",
        "    def __getitem__(self, key: int) -> str:\n",
        "        pass\n",
        "\n",
        "    @overload\n",
        "    def __getitem__(self, key: str) -> int:\n",
        "        pass\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        if isinstance(key, int):\n",
        "            return self.int_to_element[key]\n",
        "        else:\n",
        "            return self.element_to_int.get(key, self.unk_id)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, path):\n",
        "        with open(path, 'r') as file:\n",
        "            data = json.load(file)\n",
        "\n",
        "        return cls(data['elements'], unk=data['unk'], num_special=data['num_special'])\n",
        "\n",
        "    def save(self, path) -> None:\n",
        "        data = dict(\n",
        "            elements=self.int_to_element,\n",
        "            unk=self[self.unk_id],\n",
        "            num_special=self.num_special\n",
        "        )\n",
        "        with open(path, 'w') as file:\n",
        "            json.dump(data, file)\n",
        "            file.close()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.element_to_int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pT9F4IoA0kRu"
      },
      "outputs": [],
      "source": [
        "def build_frequency_dictionary(elements):\n",
        "    element_names = get_frequency_elements(elements)\n",
        "\n",
        "    element_names.insert(0, '<start>')\n",
        "    element_names.insert(1, '<unk>')\n",
        "\n",
        "    return Vocab(element_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgVzGy350nJ4"
      },
      "outputs": [],
      "source": [
        "def build_note_dictionary(elements):\n",
        "    element_names = get_frequency_elements(elements)\n",
        "\n",
        "    min_midi = None\n",
        "    max_midi = None\n",
        "\n",
        "    from music21 import note\n",
        "    for name in element_names:\n",
        "        if name == '<rest>':\n",
        "            continue\n",
        "        midi = note.Pitch(name).midi\n",
        "        if min_midi is None:\n",
        "            min_midi = max_midi = midi\n",
        "        else:\n",
        "            min_midi = min(min_midi, midi)\n",
        "            max_midi = max(max_midi, midi)\n",
        "\n",
        "    element_names = ['<start>', '<rest>']\n",
        "    for midi in range(min_midi, max_midi + 1):\n",
        "        element_names.append(note.Pitch(midi).nameWithOctave)\n",
        "\n",
        "    return Vocab(element_names, unk='<rest>')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DdxMgU90YrV"
      },
      "outputs": [],
      "source": [
        "def get_vocab_builder(channel_name: str):\n",
        "    if channel_name == 'note':\n",
        "        return build_note_dictionary\n",
        "\n",
        "    return build_frequency_dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69qS3ybGgbVR"
      },
      "outputs": [],
      "source": [
        "def build_dataset(data, vocab_map):\n",
        "    X_list = list()\n",
        "    for idx, vocab in enumerate(vocab_map.values()):\n",
        "        X = torch.nn.utils.rnn.pad_sequence(\n",
        "            [torch.from_numpy(np.asarray(\n",
        "                [vocab[x[idx]] for x in s],\n",
        "                dtype=np.int64\n",
        "            )) for s in data],\n",
        "            batch_first=True\n",
        "        )\n",
        "        X_list.append(X)\n",
        "\n",
        "    return torch.stack(X_list, dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00URYA9UMvM5"
      },
      "source": [
        "# Подготовка данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqfQAJnohm3S"
      },
      "outputs": [],
      "source": [
        "if SPLIT_DATASET:\n",
        "    split_dataset(DATA_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZ31AqWi3vqv",
        "outputId": "a679f40f-973d-479d-8c17-073d4bf7c145"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading note vocab\n",
            "Loading chord vocab\n",
            "Loading duration vocab\n",
            "Loading shift vocab\n"
          ]
        }
      ],
      "source": [
        "vocab_map = OrderedDict()\n",
        "\n",
        "if BUILD_DATASET:\n",
        "    print(\"Loading training dataset\")\n",
        "    data = load_processed_data(DATA_DIR, max_seq_len=MAX_SEQ_LEN, drop_last=True)\n",
        "\n",
        "    print(f'Dataset size {len(data)}')\n",
        "\n",
        "    for idx, name in enumerate(CHANNEL_NAMES):\n",
        "        print(f'Building {name} vocab')\n",
        "        vocab = get_vocab_builder(name)([y[idx] for x in data for y in x])\n",
        "        vocab.save(os.path.join(VOCAB_DIR, f'{name}.vocab'))\n",
        "        vocab_map[name] = vocab\n",
        "\n",
        "    print(\"Building training dataset\")\n",
        "    X_train = build_dataset(data, vocab_map)\n",
        "\n",
        "    print(\"Saving training dataset\")\n",
        "    torch.save(X_train, os.path.join(DATASET_DIR, 'train.pt'))\n",
        "\n",
        "    print(\"Building validation dataset\")\n",
        "    X_val = build_dataset(load_processed_data(DATA_DIR, split='val', max_seq_len=MAX_SEQ_LEN), vocab_map)\n",
        "\n",
        "    print(\"Saving validation dataset\")\n",
        "    torch.save(X_val, os.path.join(DATASET_DIR, 'validation.pt'))\n",
        "else:\n",
        "    for idx, name in enumerate(CHANNEL_NAMES):\n",
        "          print(f'Loading {name} vocab')\n",
        "          vocab = Vocab.load(os.path.join(VOCAB_DIR, f'{name}.vocab'))\n",
        "          vocab_map[name] = vocab\n",
        "\n",
        "    X_train = torch.load(os.path.join(DATASET_DIR, 'train.pt'))\n",
        "    X_val = torch.load(os.path.join(DATASET_DIR, 'validation.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25pV_bL-4Lsw"
      },
      "outputs": [],
      "source": [
        "class DataSet(torch.utils.data.Dataset):\n",
        "    def __init__(self, X):\n",
        "        super().__init__()\n",
        "\n",
        "        self.X = X\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgI3UBLa01QV"
      },
      "outputs": [],
      "source": [
        "class TrainDataSetIterator:\n",
        "    def __init__(self, X, min_note, max_note):\n",
        "        super().__init__()\n",
        "\n",
        "        self.X = X\n",
        "        self.min_note = min_note\n",
        "        self.max_note = max_note\n",
        "\n",
        "        self._size = len(self.X)\n",
        "        self._cursor = 0\n",
        "\n",
        "        rng = np.random.default_rng()\n",
        "        self._indices = rng.integers(self._size, size=self._size)\n",
        "\n",
        "        t = rng.random(self._size) > 0.5\n",
        "        n_t = t.sum(dtype=np.int32)\n",
        "\n",
        "        t_off = np.zeros(self._size, dtype=np.int32)\n",
        "        t_off[t] = rng.normal(size=n_t, scale=12).astype(np.int32)\n",
        "\n",
        "        self._t_off = t_off\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        cursor = self._cursor\n",
        "        if cursor >= self._size:\n",
        "            raise StopIteration()\n",
        "\n",
        "        self._cursor = cursor + 1\n",
        "\n",
        "        x = self.X[self._indices[cursor]]\n",
        "        t_off = self._t_off[cursor]\n",
        "        if t_off != 0:\n",
        "            x = self.transpose(x, t_off)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def transpose(self, x: torch.Tensor, offset):\n",
        "        notes = x[..., 0]\n",
        "        mask = notes >= self.min_note\n",
        "        max_n = notes.max().item()\n",
        "        min_n = notes[mask].min().item()\n",
        "\n",
        "        min_t = self.min_note - min_n\n",
        "        max_t = self.max_note - max_n\n",
        "\n",
        "        if offset >= max_t:\n",
        "            offset = min_t + (offset - min_t) % (max_t - min_t)\n",
        "        elif offset < min_t:\n",
        "            offset = max_t - 1 - (max_t - offset) % (max_t - min_t)\n",
        "\n",
        "        x = x.clone()\n",
        "        x[..., 0][mask] += offset\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvCEI9x903dN"
      },
      "outputs": [],
      "source": [
        "class TrainDataSet(torch.utils.data.IterableDataset):\n",
        "    def __init__(self, X, min_note, max_note):\n",
        "        super().__init__()\n",
        "\n",
        "        self.X = X\n",
        "        self.min_note = min_note\n",
        "        self.max_note = max_note\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __iter__(self):\n",
        "        return TrainDataSetIterator(self.X, self.min_note, self.max_note)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6-JuhpB09l7"
      },
      "outputs": [],
      "source": [
        "def collate_seq(batch):\n",
        "    return torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0)\n",
        "\n",
        "\n",
        "def collate(batch):\n",
        "    return collate_seq(batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9bOa6MgNc24"
      },
      "source": [
        "# Модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67xWcM2btVeU"
      },
      "outputs": [],
      "source": [
        "class Encoder(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 channel_names: Sequence[int],\n",
        "                 embedding_sizes: int,\n",
        "                 embedding_dims: int,\n",
        "                 layers_filters: Sequence[int],\n",
        "                 latent_dim: int = 128,\n",
        "                 seq_len: int = 128,\n",
        "                 dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.seq_len = seq_len\n",
        "        self.channel_names = channel_names\n",
        "        \n",
        "        self.embeddings = nn.ModuleDict([(channel, nn.Embedding(num_embeddings=embedding_sizes[idx], embedding_dim=embedding_dims[idx])) for idx, channel in enumerate(channel_names)])\n",
        "\n",
        "        modules = []\n",
        "\n",
        "        in_channels = sum(embedding_dims)\n",
        "\n",
        "        for filters in layers_filters:\n",
        "            modules.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Dropout(dropout),\n",
        "                    nn.Conv1d(in_channels, out_channels=filters,\n",
        "                                kernel_size=4, stride=2, padding=1),\n",
        "                    nn.ELU())\n",
        "            )\n",
        "\n",
        "            in_channels = filters\n",
        "\n",
        "        self.encoder = nn.Sequential(*modules)\n",
        "        self.fc_mu = nn.Linear(in_channels * (seq_len // (2**len(layers_filters))), latent_dim)\n",
        "        self.fc_var = nn.Linear(in_channels * (seq_len // (2**len(layers_filters))), latent_dim)\n",
        "        \n",
        "    def forward(self, x: torch.Tensor):\n",
        "        emb = []\n",
        "        for idx, channel in enumerate(self.channel_names):\n",
        "            emb.append(self.embeddings[channel](x[..., idx]))\n",
        "        \n",
        "        x = torch.cat(emb, dim=-1)\n",
        "\n",
        "        x = x.permute((0, 2, 1))\n",
        "\n",
        "        x = self.encoder(x)\n",
        "\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "\n",
        "        mu = self.fc_mu(x)\n",
        "        log_var = self.fc_var(x)\n",
        "\n",
        "        std = torch.exp(0.5 * log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "\n",
        "        return eps * std + mu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56qfI7xO0n9J"
      },
      "outputs": [],
      "source": [
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(self, \n",
        "                 embedding_sizes: Sequence[int],\n",
        "                 layers_filters: Sequence[int],\n",
        "                 latent_dim: int = 128,\n",
        "                 seq_len: int = 128, \n",
        "                 dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding_sizes = embedding_sizes\n",
        "        channels_num = sum(embedding_sizes)\n",
        "\n",
        "        unflatten_len = seq_len // 2**len(layers_filters)\n",
        "\n",
        "        in_channels = unflatten_len * layers_filters[-1]\n",
        "\n",
        "        self.decoder_input = nn.Linear(latent_dim, in_channels)\n",
        "\n",
        "        self.unflatten = nn.Unflatten(1, (layers_filters[-1], unflatten_len))\n",
        "        \n",
        "        in_channels = layers_filters[-1]\n",
        "\n",
        "        modules = []\n",
        "\n",
        "        for filters in reversed(layers_filters[:-1]):\n",
        "            modules.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Dropout(dropout),\n",
        "                    nn.ConvTranspose1d(in_channels,\n",
        "                                        filters,\n",
        "                                        kernel_size=4,\n",
        "                                        stride = 2,\n",
        "                                        padding = 1),\n",
        "                    nn.ELU())\n",
        "            )\n",
        "\n",
        "            in_channels = filters\n",
        "\n",
        "        modules.append(\n",
        "            nn.Sequential(\n",
        "                nn.Dropout(dropout),\n",
        "                nn.ConvTranspose1d(in_channels,\n",
        "                                    channels_num,\n",
        "                                    kernel_size=4,\n",
        "                                    stride = 2,\n",
        "                                    padding = 1))\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(*modules)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, rng: Optional[torch.Generator] = None):\n",
        "        x = self.decoder_input(x)\n",
        "\n",
        "        x = self.unflatten(x)\n",
        "\n",
        "        x = self.decoder(x)\n",
        "\n",
        "        x_list = list()\n",
        "\n",
        "        for x in torch.split(x, self.embedding_sizes, dim=1):\n",
        "            x_list.append(nn.functional.log_softmax(x, dim=1))\n",
        "\n",
        "        return x_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMYaJfp3jD-_"
      },
      "outputs": [],
      "source": [
        "class Generator(torch.nn.Module):\n",
        "    def __init__(self, \n",
        "                 *, \n",
        "                 channel_names: Sequence[str], \n",
        "                 embedding_sizes: Sequence[int],\n",
        "                 embedding_dims: Sequence[int],\n",
        "                 filters: Sequence[int],\n",
        "                 latent_dim: int = 128,\n",
        "                 seq_len: int = 128,\n",
        "                 dropout=0.1):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.channel_names = channel_names\n",
        "\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        self.encoder = Encoder(channel_names, embedding_sizes, embedding_dims, filters, latent_dim, seq_len, dropout)\n",
        "        self.decoder = Decoder(embedding_sizes, filters, latent_dim, seq_len, dropout)\n",
        "        \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def make_variations(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.forward(x)\n",
        "\n",
        "        pred_list = []\n",
        "\n",
        "        for channel in x:\n",
        "            channel = channel.permute((0, 2, 1))\n",
        "            channel = channel.argmax(dim=-1)\n",
        "            pred_list.append(channel)\n",
        "\n",
        "        return torch.stack(pred_list, dim=-1)\n",
        "\n",
        "    def predict(self, batch_size) -> torch.Tensor:\n",
        "        device = next(self.parameters()).device\n",
        "\n",
        "        noise = torch.rand(batch_size, self.latent_dim).to(device)\n",
        "\n",
        "        channels = self.decoder(noise)\n",
        "\n",
        "        pred_list = []\n",
        "\n",
        "        for channel in channels:\n",
        "            channel = channel.permute((0, 2, 1))\n",
        "            channel = channel.argmax(dim=-1)\n",
        "            pred_list.append(channel)\n",
        "\n",
        "        return torch.stack(pred_list, dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xbkpd_4wNqme"
      },
      "source": [
        "# Объявление методов обучения"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZSBKDIY1vm7"
      },
      "outputs": [],
      "source": [
        "def make_midi(x: torch.Tensor, vocab_map: OrderedDict):\n",
        "    from music21 import chord, duration, instrument, note, stream\n",
        "    from fractions import Fraction\n",
        "\n",
        "    midi_stream = stream.Stream()\n",
        "\n",
        "    x = x.detach().cpu().numpy()\n",
        "    next_offset = None\n",
        "    for line in x:\n",
        "        if np.any(line <= 1):\n",
        "            continue\n",
        "\n",
        "        p, c, d, s = (v[int(x)] for v, x in zip(vocab_map.values(), line))\n",
        "\n",
        "        if p == '<rest>':\n",
        "            element = note.Rest()\n",
        "        else:\n",
        "            if c == '0':\n",
        "                element = note.Note(p)\n",
        "            else:\n",
        "                p = note.pitch.Pitch(p).midi\n",
        "                note_list = [\n",
        "                    note.Note(p + int(x)) for x in c.split('-')\n",
        "                ]\n",
        "                element = chord.Chord(note_list)\n",
        "\n",
        "            element.storedInstrument = instrument.Piano()\n",
        "\n",
        "        element.duration = duration.Duration(Fraction(d))\n",
        "\n",
        "        midi_stream.append(element)\n",
        "        if next_offset is not None:\n",
        "            element.offset = next_offset\n",
        "            next_offset = None\n",
        "\n",
        "        if s != 'f':\n",
        "            next_offset = Fraction(element.offset) + Fraction(s)\n",
        "\n",
        "    return midi_stream\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAJ0tVzhIDdt"
      },
      "outputs": [],
      "source": [
        "def train(*, \n",
        "          start_epoch: int,\n",
        "          epoch_count: int, \n",
        "          device: torch.device,\n",
        "          generator: Generator,\n",
        "          generator_optimizer,\n",
        "          train_data_loader: TrainDataSet,\n",
        "          val_data_loader: DataSet,\n",
        "          summary_writer: SummaryWriter):\n",
        "    \n",
        "    def save(e):\n",
        "        if generator_optimizer is not None:\n",
        "            torch.save(generator.state_dict(),\n",
        "                        os.path.join(CHECKPOINT_DIR, f'generator-checkpoint-{e + 1}.model'))\n",
        "            torch.save(generator_optimizer.state_dict(),\n",
        "                        os.path.join(CHECKPOINT_DIR, f'generator-checkpoint-{e + 1}.optimizer'))\n",
        "\n",
        "    batch_index = 0\n",
        "\n",
        "    for epoch in range(start_epoch, start_epoch + epoch_count):\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "        metrics = dict([(channel, 0) for channel in CHANNEL_NAMES])\n",
        "\n",
        "        epoch_loss = 0\n",
        "        train_samples = 0\n",
        "\n",
        "        generator.train()\n",
        "\n",
        "        for y_batch in tqdm(train_data_loader):\n",
        "            y_batch = y_batch.to(device)\n",
        "\n",
        "            generator_optimizer.zero_grad()\n",
        "            pred = generator(y_batch)\n",
        "            \n",
        "            loss = None\n",
        "\n",
        "            for idx, channel in enumerate(CHANNEL_NAMES):\n",
        "                channel_loss = F.nll_loss(pred[idx], y_batch[..., idx])                \n",
        "                loss = loss + channel_loss if loss is not None else channel_loss\n",
        "                \n",
        "                metrics[channel] += channel_loss.item() * y_batch.shape[0]\n",
        "                summary_writer.add_scalar(f'batch_loss/{channel}', channel_loss.item(), batch_index, new_style=True)\n",
        "\n",
        "            summary_writer.add_scalar(f'batch_loss/total', loss.item(), batch_index, new_style=True)\n",
        "\n",
        "            epoch_loss += loss.item() * y_batch.shape[0]\n",
        "            \n",
        "            loss.backward()\n",
        "            \n",
        "            torch.nn.utils.clip_grad_norm_(generator.parameters(), 2.0)\n",
        "            generator_optimizer.step()\n",
        "\n",
        "            train_samples += y_batch.shape[0]\n",
        "            batch_index += 1\n",
        "\n",
        "        for item in metrics.items():\n",
        "            channel_loss = item[1] / train_samples\n",
        "            summary_writer.add_scalar(f'epoch_loss/{item[0]}', channel_loss, epoch, new_style=True)\n",
        "        \n",
        "        epoch_loss /= train_samples\n",
        "        summary_writer.add_scalar('epoch_loss/total', epoch_loss, epoch, new_style=True)\n",
        "\n",
        "        val_loss, val_acc = validate(epoch, device, generator, val_data_loader, summary_writer)\n",
        "\n",
        "        print(f'cuda memory {torch.cuda.memory_stats()[\"allocated_bytes.all.peak\"] // (1024 * 1024)} MB')\n",
        "        print(f'Epoch {epoch + 1}; loss {epoch_loss}; val_loss {val_loss}; val_acc {val_acc}')\n",
        "\n",
        "        print('save checkpoint')\n",
        "        save(epoch)\n",
        "        \n",
        "        if (epoch + 1) % 10 == 0 or epoch + 1 == start_epoch + epoch_count:\n",
        "            print('clean up unnecessary сheckpoints')\n",
        "            os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "            for row in os.listdir(CHECKPOINT_DIR):\n",
        "                try:\n",
        "                    if int(row.split('-')[-1].split('.')[0]) % 5 != 0:\n",
        "                        os.remove(os.path.join(CHECKPOINT_DIR, row))\n",
        "                except:\n",
        "                    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ct2ZcXzrMRf-"
      },
      "outputs": [],
      "source": [
        "@torch.inference_mode()\n",
        "def validate(epoch: int,\n",
        "             device: torch.device,\n",
        "             generator: Generator,\n",
        "             data_loader: torch.utils.data.DataLoader,\n",
        "             summary_writer: SummaryWriter):\n",
        "\n",
        "    loss_metrics = dict([(channel, 0) for channel in CHANNEL_NAMES])\n",
        "    acc_metrics = dict([(channel, 0) for channel in CHANNEL_NAMES])\n",
        "    epoch_loss = 0.\n",
        "    epoch_acc = 0.\n",
        "    samples = 0.\n",
        "\n",
        "    generator.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for y_batch in tqdm(data_loader):\n",
        "            y_batch = y_batch.to(device)\n",
        "\n",
        "            pred = generator(y_batch)\n",
        "            \n",
        "            # calculate loss\n",
        "            \n",
        "            loss = 0\n",
        "\n",
        "            for idx, channel in enumerate(CHANNEL_NAMES):\n",
        "                channel_loss = F.nll_loss(pred[idx], y_batch[..., idx]).item()\n",
        "                loss_metrics[channel] += channel_loss * y_batch.shape[0]\n",
        "                loss += channel_loss\n",
        "            \n",
        "            epoch_loss += loss * y_batch.shape[0]\n",
        "            \n",
        "            # calculate acc\n",
        "            \n",
        "            pred_list = []\n",
        "            \n",
        "            for channel in pred:\n",
        "                channel = channel.permute((0, 2, 1))\n",
        "                channel = channel.argmax(dim=-1)\n",
        "                pred_list.append(channel)\n",
        "                \n",
        "            pred = torch.stack(pred_list, dim=-1)\n",
        "            \n",
        "            del pred_list\n",
        "            \n",
        "            for idx, channel in enumerate(CHANNEL_NAMES):\n",
        "                channel_acc = (pred[..., idx] == y_batch[..., idx]).float().sum().item() / y_batch.shape[1]\n",
        "                acc_metrics[channel] += channel_acc \n",
        "                \n",
        "            epoch_acc += (pred == y_batch).float().min(dim=-1)[0].sum().item() / y_batch.shape[1]\n",
        "\n",
        "            samples += y_batch.shape[0]\n",
        "\n",
        "    for item in loss_metrics.items():\n",
        "        channel_loss = item[1] / samples\n",
        "        summary_writer.add_scalar(f'val_loss/{item[0]}', channel_loss, epoch, new_style=True)\n",
        "\n",
        "    epoch_loss /= samples\n",
        "    summary_writer.add_scalar('val_loss/total', epoch_loss, epoch, new_style=True)\n",
        "\n",
        "    for item in acc_metrics.items():\n",
        "        channel_acc = item[1] / samples\n",
        "        summary_writer.add_scalar(f'val_acc/{item[0]}', channel_acc, epoch, new_style=True)\n",
        "\n",
        "    epoch_acc /= samples\n",
        "    summary_writer.add_scalar('val_acc/total', epoch_acc, epoch, new_style=True)\n",
        "\n",
        "    return epoch_loss, epoch_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efzdrhPcN6KR"
      },
      "source": [
        "# Обучение"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfsUK5OX2SM2"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available else \"cpu\")\n",
        "\n",
        "generator = Generator(\n",
        "    channel_names=CHANNEL_NAMES,\n",
        "    embedding_sizes=tuple(len(v) for v in vocab_map.values()),\n",
        "    embedding_dims=(48, 128, 32, 32),\n",
        "    filters=[384, 512, 256, 256],\n",
        "    latent_dim=2048,\n",
        "    dropout=0.15,\n",
        ")\n",
        "\n",
        "if START_EPOCH > 0:\n",
        "    generator.load_state_dict(torch.load(\n",
        "        os.path.join(CHECKPOINT_DIR, f'generator-checkpoint-{START_EPOCH}.model'),\n",
        "        map_location='cpu'\n",
        "    ))\n",
        "\n",
        "generator.to(device)\n",
        "\n",
        "generator_optimizer = torch.optim.AdamW(generator.parameters(), lr=LEARNING_RATE)\n",
        "if START_EPOCH > 0:\n",
        "    generator_optimizer.load_state_dict(torch.load(\n",
        "        os.path.join(CHECKPOINT_DIR, f'generator-checkpoint-{START_EPOCH}.optimizer'),\n",
        "        map_location='cpu'\n",
        "    ))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puxbMOyjPB34"
      },
      "outputs": [],
      "source": [
        "batch_size = BATCH_SIZE\n",
        "epoch_count = EPOCH_COUNT\n",
        "\n",
        "train_data_loader = torch.utils.data.DataLoader(TrainDataSet(X_train, 3, len(vocab_map['note'])),\n",
        "                                                batch_size, shuffle=False,\n",
        "                                                collate_fn=collate,\n",
        "                                                drop_last=True,\n",
        "                                                pin_memory=True)\n",
        "\n",
        "val_data_loader = torch.utils.data.DataLoader(DataSet(X_val),\n",
        "                                                batch_size * 4, shuffle=False,\n",
        "                                                collate_fn=collate,\n",
        "                                                pin_memory=True)\n",
        "\n",
        "\n",
        "run_name = f'run-{datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")}'\n",
        "summary_writer = SummaryWriter(log_dir=os.path.join('logs', run_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLe98cQr48J_",
        "outputId": "9b991bff-9dc5-440e-f7c5-d65f6864bdef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████████████████████████████| 707/707 [05:07<00:00,  2.30it/s]\n",
            "100%|██████████████████████████████████████████████████████████████████| 22/22 [00:10<00:00,  2.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda memory 1980 MB\n",
            "Epoch 108; loss 6.91065745711158; val_loss 6.58783307787958; val_acc 0.09766090626719237\n",
            "save checkpoint\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████████████████████████████| 707/707 [05:06<00:00,  2.31it/s]\n",
            "100%|██████████████████████████████████████████████████████████████████| 22/22 [00:10<00:00,  2.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda memory 1980 MB\n",
            "Epoch 109; loss 6.8941836728064985; val_loss 6.576181836845721; val_acc 0.09825010315422703\n",
            "save checkpoint\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████████████████████████████| 707/707 [05:08<00:00,  2.29it/s]\n",
            "100%|██████████████████████████████████████████████████████████████████| 22/22 [00:10<00:00,  2.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda memory 1980 MB\n",
            "Epoch 110; loss 6.887736999668191; val_loss 6.565468013210077; val_acc 0.09857711638089126\n",
            "save checkpoint\n",
            "clean up unnecessary сheckpoints\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████████████████████████████| 707/707 [05:07<00:00,  2.30it/s]\n",
            "100%|██████████████████████████████████████████████████████████████████| 22/22 [00:10<00:00,  2.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda memory 1980 MB\n",
            "Epoch 111; loss 6.875609531429566; val_loss 6.554775637223177; val_acc 0.09869388123509995\n",
            "save checkpoint\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|█████████████████████████▉                                      | 286/707 [02:05<03:06,  2.26it/s]"
          ]
        }
      ],
      "source": [
        "print(\"Training\")\n",
        "train(\n",
        "    start_epoch=START_EPOCH,\n",
        "    epoch_count=EPOCH_COUNT,\n",
        "    device=device,\n",
        "    generator=generator,\n",
        "    generator_optimizer=generator_optimizer,\n",
        "    train_data_loader=train_data_loader,\n",
        "    val_data_loader=val_data_loader,\n",
        "    summary_writer=summary_writer\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjRA_iWgN_Cx"
      },
      "source": [
        "# Тестирование"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-ukSovSSlS4"
      },
      "outputs": [],
      "source": [
        "with torch.inference_mode():\n",
        "    generator.eval()\n",
        "    music_batch = generator.predict(batch_size=8)\n",
        "\n",
        "os.makedirs(GENERATED_DIR, exist_ok=True)\n",
        "for idx in range(music_batch.shape[0]):\n",
        "    midi = make_midi(music_batch[idx], vocab_map)\n",
        "    midi.write('midi', os.path.join(GENERATED_DIR, f'midi-{idx}.midi'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtNhO-Zk8du_"
      },
      "outputs": [],
      "source": [
        "music_batch.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5n4_E3LaYw7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}